{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Motifs analysis - Part 1: Motifs extraction\n",
    "\n",
    "Notebook to perform the discriminative motifs analysis. It requires a trained model but it is an independant analysis from the analysis of feature space and from the prototypes analysis.\n",
    "\n",
    "Motifs are extracted on the base of Class-Activation Maps (CAMs) which display the saliency of a class in a given input according to a model. CAMs towards any class can be computed regardless of the actual class of the input. This means that one can look for discriminative motifs of class B in an input of class A. However, for the sake of motif extraction, we don't use this feature of CAMs. Instead we produce CAMs towards the actual class of the input.\n",
    "\n",
    "The motif extraction procedure is as follow:\n",
    "1. Select trajectories from which to extract motifs.\n",
    "2. Compute CAM for each trajectory (saliency towards its own class).\n",
    "3. Binarize each time point into 'relevant' and 'non-relevant' to recognize input class.\n",
    "4. Optional but recommended, extend the 'relevant' regions to capture more context around the motifs and connect smaller adjacents motifs into a bigger one. Also filter for motif length.\n",
    "5. Extract the longest 'relevant' stretches of time-points. These are the final motifs.\n",
    "\n",
    "In order to visualize these motifs, we propose to cluster them afterwards as follow:\n",
    "1. Build a distance matrix between the motifs with dynamic time warping (dtw)\n",
    "2. Cluster with hierarchical clustering.\n",
    "3. Visualize dynamics captured by each cluster.\n",
    "\n",
    "This clustering can be run in 2 modes: either patterns from every class are pooled together, either a separate clustering is run indepently for each class. In the 1st case, this will reflect the diversity of patterns at the dataset level and can reveal dynamics overlap between classes. In the second case, the emphasis is put on the diversity of dynamics induced by each class.\n",
    "\n",
    "\n",
    "This notebook covers only the motif extraction part. It ends with the export of the motifs to a csv file. Go to the next one for computing DTW and clustering!\n",
    "\n",
    "\n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.filters import threshold_li, threshold_mean\n",
    "import os\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "# Custom functions/classes\n",
    "path_to_module = '../source'  # Path where all the .py files are, relative to the notebook folder\n",
    "sys.path.append(path_to_module)\n",
    "from load_data import DataProcesser\n",
    "from results_model import top_confidence_perclass, least_correlated_set\n",
    "from pattern_utils import extend_segments, create_cam, longest_segments, extract_pattern\n",
    "from class_dataset import myDataset, ToTensor, RandomCrop\n",
    "\n",
    "# For reproducibility\n",
    "myseed = 7\n",
    "torch.manual_seed(myseed)\n",
    "torch.cuda.manual_seed(myseed)\n",
    "np.random.seed(myseed)\n",
    "\n",
    "cuda_available = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Parameters for the motifs extraction:\n",
    "- selected_set: str one of ['all', 'training', 'validation', 'test'], from which set of trajectories should motifs be extracted? For this purprose, extracting from training data also makes sense.\n",
    "- n_series_perclass: int, maximum number of series, per class, on which motif extraction is attempted.\n",
    "- n_pattern_perseries: int, maximum number of motifs to extract out of a single trajectory.\n",
    "- mode_series_selection: str one of ['top_confidence', 'least_correlated']. Mode to select the trajectories from which to extract the motifs (see Prototype analysis). If top confidence, the motifs might be heavily biased towards a representative subpopulation of the class. Hence, the output might not reflect the whole diversity of motifs induced by the class.\n",
    "- extend_patt: int, by how many points to extend motifs? After binarization into 'relevant' and 'non-relevant time points', the motifs are usually fragmented because a few points in their middle are improperly classified as 'non-relevant'. This parameter allows to extend each fragment by a number of time points (in both time directions) before extracting the actual patterns.\n",
    "- min_len_patt/max_len_patt: int, set minimum/maximum size of a motif. **/!\\ The size is given in number of time-points. This means that if the input has more than one channel, the actual length of the motifs will be divided across them.** For example, a motif that spans over 2 channels for 10 time points will be considered of length 20.\n",
    "\n",
    "Parameters for the groups of motifs:\n",
    "- export_perClass: bool, whether to run the motif clustering class per class.\n",
    "- export_allPooled: bool, whether to pool all motifs across classes for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_set = 'all'\n",
    "n_series_perclass = 50\n",
    "n_pattern_perseries = 1\n",
    "mode_series_selection = 'top_confidence'\n",
    "# mode_series_selection = 'least_correlated'\n",
    "thresh_confidence = 0.5  # used in least_correlated mode to choose set of series with minimal classification confidence\n",
    "extend_patt = 0\n",
    "min_len_patt = 0\n",
    "max_len_patt = 200 # length to divide by nchannel\n",
    "\n",
    "export_perClass = False\n",
    "export_allPooled = True\n",
    "\n",
    "assert selected_set in ['all', 'training', 'validation', 'test']\n",
    "assert mode_series_selection in ['top_confidence', 'least_correlated']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and data\n",
    "\n",
    "- Pay attention to the order of 'meas_var', should be the same as for training the model!\n",
    "- Pay attention to trajectories preprocessing.\n",
    "- Set batch_size as high as memory allows for speed up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '../sample_data/Synthetic_Univariate.zip'\n",
    "model_file = 'models/FRST/sampleModel_Synthetic_Univariate.pytorch'\n",
    "# data_file = '../sample_data/GrowthFactor_ErkAkt_Bivariate.zip'\n",
    "# model_file = 'models/ERK_AKT/sampleModel_GrowthFactor_ErkAkt_Bivariate.pytorch'\n",
    "\n",
    "out_dir = 'auto'  # If 'auto' will automatically create a directory to save motifs tables\n",
    "\n",
    "meas_var = None  # Set to None for auto detection\n",
    "start_time = None  # Set to None for auto detection\n",
    "end_time = None  # Set to None for auto detection\n",
    "\n",
    "batch_size = 32  # Set as high as memory allows for speed up\n",
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if is_cuda else 'cpu')\n",
    "model = torch.load(model_file) if cuda_available else torch.load(model_file, map_location='cpu')\n",
    "model.eval()\n",
    "model.double()\n",
    "model.batch_size = batch_size\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay attention that **data.process() is already centering the data**, so don't do a second time when loading the data in the DataLoader. The **random crop** should be performed before passing the trajectories to the model to ensure that the same crop is used as input and for extracting the patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../source/load_data.py:88: UserWarning: datatable module not found, using pandas instead. To prevent this message from appearing use \"datatable = False\" when reading the archive.\n",
      "  warnings.warn('datatable module not found, using pandas instead. To prevent this message from appearing'\n",
      "../source/load_data.py:499: UserWarning: Data were already preprocessed, be careful not to process again with dataloaders.\n",
      "  warnings.warn('Data were already preprocessed, be careful not to process again with dataloaders.')\n"
     ]
    }
   ],
   "source": [
    "# Transformations to perform when loading data into the model\n",
    "ls_transforms = transforms.Compose([RandomCrop(output_size=model.length, ignore_na_tails=True),\n",
    "                                                            ToTensor()])\n",
    "# Loading and PREPROCESSING\n",
    "data = DataProcesser(data_file)\n",
    "meas_var = data.detect_groups_times()['groups'] if meas_var is None else meas_var\n",
    "start_time = data.detect_groups_times()['times'][0] if start_time is None else start_time\n",
    "end_time = data.detect_groups_times()['times'][1] if end_time is None else end_time\n",
    "# Path where to export tables with motifs\n",
    "if out_dir == 'auto':\n",
    "    out_dir = 'output/' + '_'.join(meas_var) + '/local_motifs/'\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "data.subset(sel_groups=meas_var, start_time=start_time, end_time=end_time)\n",
    "cols_to_check=data.dataset.columns.values[data.dataset.columns.str.startswith('FGF')]\n",
    "cols_dict={k:'float64' for k in cols_to_check}\n",
    "data.dataset=data.dataset.astype(cols_dict)\n",
    "data.get_stats()\n",
    "data.process(method='center_train', independent_groups=True)  # do here and not in loader so can use in df\n",
    "data.crop_random(model.length, ignore_na_tails=True)\n",
    "data.split_sets(which='dataset')\n",
    "classes = tuple(data.classes[data.col_classname])\n",
    "dict_classes = data.classes[data.col_classname]\n",
    "\n",
    "# Random crop before to keep the same in df as the ones passed in the model\n",
    "if selected_set == 'validation':\n",
    "    selected_data = myDataset(dataset=data.validation_set, transform=ls_transforms)\n",
    "    df = data.validation_set\n",
    "elif selected_set == 'training':\n",
    "    selected_data = myDataset(dataset=data.train_set, transform=ls_transforms)\n",
    "    df = data.train_set\n",
    "elif selected_set == 'test':\n",
    "    selected_data = myDataset(dataset=data.test_set, transform=ls_transforms)\n",
    "    df = data.train_set\n",
    "elif selected_set == 'all':\n",
    "    try:\n",
    "        selected_data = myDataset(dataset=data.dataset_cropped, transform=ls_transforms)\n",
    "        df = data.dataset_cropped\n",
    "    except:\n",
    "        selected_data = myDataset(dataset=data.dataset, transform=ls_transforms)\n",
    "        df = data.dataset\n",
    "\n",
    "if batch_size > len(selected_data):\n",
    "    raise ValueError('Batch size ({}) must be smaller than the number of trajectories in the selected set ({}).'.format(batch_size, len(selected_data)))        \n",
    "\n",
    "data_loader = DataLoader(dataset=selected_data,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True,\n",
    "                         num_workers=4)\n",
    "# Dataframe used for retrieving trajectories. wide_to_long() instead of melt() because can do melting per group of columns\n",
    "df = pd.wide_to_long(df, stubnames=meas_var, i=[data.col_id, data.col_class], j='Time', sep='_', suffix='\\d+')\n",
    "df = df.reset_index()  # wide_to_long creates a multi-level Index, reset index to retrieve indexes in columns\n",
    "df.rename(columns={data.col_id: 'ID', data.col_class: 'Class'}, inplace=True)\n",
    "df['ID'] = df['ID'].astype('U32')\n",
    "del data  # free memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select trajectories from which to extract patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 120/125 [00:02<00:00, 57.03it/s]"
     ]
    }
   ],
   "source": [
    "if mode_series_selection == 'least_correlated':\n",
    "    set_trajectories = least_correlated_set(model, data_loader, threshold_confidence=thresh_confidence, device=device,\n",
    "                                            n=n_series_perclass, labels_classes=dict_classes)\n",
    "elif mode_series_selection == 'top_confidence':\n",
    "    set_trajectories = top_confidence_perclass(model, data_loader, device=device, n=n_series_perclass,\n",
    "                                               labels_classes=dict_classes)\n",
    "\n",
    "# free some memory by keeping only relevant series\n",
    "selected_trajectories = set_trajectories['ID']\n",
    "df = df[df['ID'].isin(selected_trajectories)]\n",
    "# Make sure that class is an integer (especially when 0 or 1, could be read as boolean)\n",
    "df['Class'] = df['Class'].astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract patterns\n",
    "\n",
    "### Extract, extend and filter patterns. \n",
    "\n",
    "Outputs a report of how many trajectories were filtered out by size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|▊         | 8/100 [00:00<00:01, 78.59it/s]\u001b[A\n",
      " 16%|█▌        | 16/100 [00:00<00:01, 74.72it/s]\u001b[A\n",
      " 24%|██▍       | 24/100 [00:00<00:01, 73.17it/s]\u001b[A\n",
      " 32%|███▏      | 32/100 [00:00<00:00, 72.95it/s]\u001b[A\n",
      " 40%|████      | 40/100 [00:00<00:00, 72.16it/s]\u001b[A\n",
      " 48%|████▊     | 48/100 [00:00<00:00, 69.79it/s]\u001b[A\n",
      " 55%|█████▌    | 55/100 [00:00<00:00, 67.84it/s]\u001b[A\n",
      " 62%|██████▏   | 62/100 [00:00<00:00, 67.04it/s]\u001b[A\n",
      " 69%|██████▉   | 69/100 [00:01<00:00, 64.87it/s]\u001b[A\n",
      " 76%|███████▌  | 76/100 [00:01<00:00, 63.62it/s]\u001b[A\n",
      " 83%|████████▎ | 83/100 [00:01<00:00, 62.42it/s]\u001b[A\n",
      " 90%|█████████ | 90/100 [00:01<00:00, 58.13it/s]\u001b[A\n",
      " 96%|█████████▌| 96/100 [00:01<00:00, 54.61it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Total number of patterns': 100, 'Number of patterns above maximum length': 0, 'Number of patterns below minimum length': 0}\n"
     ]
    }
   ],
   "source": [
    "# Initialize dict to store the patterns and set progress bar\n",
    "store_patts = {i:[] for i in classes}\n",
    "model.batch_size = 1  # Leave it to 1!\n",
    "report_filter = {'Total number of patterns': 0,\n",
    "                 'Number of patterns above maximum length': 0,\n",
    "                 'Number of patterns below minimum length': 0}\n",
    "pbar = tqdm(total=len(selected_trajectories))\n",
    "\n",
    "for id_trajectory in selected_trajectories:\n",
    "    # Read and format the trajectories to numpy\n",
    "    series_numpy = np.array(df.loc[df['ID'] == id_trajectory][meas_var]).astype('float').squeeze()\n",
    "    # Row: measurement; Col: time\n",
    "    if len(meas_var) >= 2:\n",
    "        series_numpy = series_numpy.transpose()\n",
    "    series_tensor = torch.tensor(series_numpy)\n",
    "    class_trajectory = df.loc[df['ID']==id_trajectory]['Class'].iloc[0]  # repeated value through all series\n",
    "    class_label = classes[class_trajectory]\n",
    "    \n",
    "    # Create and process the CAM for the trajectory\n",
    "    cam = create_cam(model, array_series=series_tensor, feature_layer='features',\n",
    "                         device=device, clip=0, target_class=class_trajectory)\n",
    "    thresh = threshold_li(cam)\n",
    "    bincam = np.where(cam >= thresh, 1, 0)\n",
    "    bincam_ext = extend_segments(array=bincam, max_ext=extend_patt)\n",
    "    patterns = longest_segments(array=bincam_ext, k=n_pattern_perseries)\n",
    "    \n",
    "    # Filter short/long patterns\n",
    "    report_filter['Total number of patterns'] += len(patterns)\n",
    "    report_filter['Number of patterns above maximum length'] += len([k for k in patterns.keys() if patterns[k] > max_len_patt])\n",
    "    report_filter['Number of patterns below minimum length'] += len([k for k in patterns.keys() if patterns[k] < min_len_patt])\n",
    "    patterns = {k: patterns[k] for k in patterns.keys() if (patterns[k] >= min_len_patt and\n",
    "                                                            patterns[k] <= max_len_patt)}\n",
    "    if len(patterns) > 0:\n",
    "        for pattern_position in list(patterns.keys()):\n",
    "            store_patts[class_label].append(extract_pattern(series_numpy, pattern_position, NA_fill=False))\n",
    "    pbar.update(1)\n",
    "\n",
    "print(report_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump patterns into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:19<00:00, 57.03it/s]\n",
      "100%|██████████| 100/100 [00:17<00:00, 54.61it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "if export_allPooled:\n",
    "    concat_patts_allPooled = np.full((sum(map(len, store_patts.values())), len(meas_var) * max_len_patt), np.nan)\n",
    "    irow = 0\n",
    "for classe in classes:\n",
    "    concat_patts = np.full((len(store_patts[classe]), len(meas_var) * max_len_patt), np.nan)\n",
    "    for i, patt in enumerate(store_patts[classe]):\n",
    "        if len(meas_var) == 1:\n",
    "            len_patt = len(patt)\n",
    "            concat_patts[i, 0:len_patt] = patt\n",
    "        if len(meas_var) >= 2:\n",
    "            len_patt = patt.shape[1]\n",
    "            for j in range(len(meas_var)):\n",
    "                offset = j*max_len_patt\n",
    "                concat_patts[i, (0+offset):(len_patt+offset)] = patt[j, :]\n",
    "    if len(meas_var) == 1:\n",
    "        headers = ','.join([meas_var[0] + '_' + str(k) for k in range(max_len_patt)])\n",
    "        fout_patt = out_dir + 'motif_{}.csv.gz'.format(classe)\n",
    "        if export_perClass:\n",
    "            np.savetxt(fout_patt, concat_patts,\n",
    "                       delimiter=',', header=headers, comments='')\n",
    "    elif len(meas_var) >= 2:\n",
    "        headers = ','.join([meas + '_' + str(k) for meas in meas_var for k in range(max_len_patt)])\n",
    "        fout_patt = out_dir + 'motif_{}.csv.gz'.format(classe)\n",
    "        if export_perClass:\n",
    "            np.savetxt(fout_patt, concat_patts,\n",
    "                       delimiter=',', header=headers, comments='')\n",
    "    if export_allPooled:\n",
    "        concat_patts_allPooled[irow:(irow+concat_patts.shape[0]), :] = concat_patts\n",
    "        irow += concat_patts.shape[0]\n",
    "\n",
    "if export_allPooled:\n",
    "    concat_patts_allPooled = pd.DataFrame(concat_patts_allPooled)\n",
    "    concat_patts_allPooled.columns = headers.split(',')\n",
    "    pattID_col = [[classe] * len(store_patts[classe]) for classe in classes]\n",
    "    concat_patts_allPooled['pattID'] = [j+'_'+str(i) for i,j in enumerate(list(chain.from_iterable(pattID_col)))]\n",
    "    concat_patts_allPooled.set_index('pattID', inplace = True)\n",
    "    fout_patt = out_dir + 'motif_allPooled.csv.gz'.format(classe)\n",
    "    concat_patts_allPooled.to_csv(fout_patt, header=True, index=True, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build distance matrix between patterns with DTW\n",
    "\n",
    "This is done in R with the implementation of the *parallelDist* package. It is very efficient and has support for multivariate cases.\n",
    "\n",
    "Check next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
